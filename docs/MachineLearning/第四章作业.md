# 《机器学习》第四章作业

## 4.1

**证明**：假设在题目条件下，不存在于与训练集一致的决策树，那么一定存在一个中间节点使得经过此节点后分出的叶子节点中包含不一样标签的样本。而由于不含冲突数据，因此在此叶子节点中标签不同的样本一定还有特征是不相同的，也就是说，此叶子节点还可以根据不同的属性进行下一步决策，使得标签不同的样本完全分离，从而使得训练误差为0。因此得证：**对于不含冲突数据的训练集，必存在与训练集一致的决策树。**

## 4.3

### 1. 准备数据

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('./watermelon3.csv', index_col=0)  # 西瓜数据集3.0
for label in df.columns:  # 离散值预处理为整数
    if df[label].dtype == object:
        df[label] = LabelEncoder().fit(df[label]).transform(df[label])
df
```

|      | 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 |  密度 | 含糖率 | 好瓜 |
| ---: | ---: | ---: | ---: | ---: | ---: | ---: | ----: | -----: | ---: |
| 编号 |      |      |      |      |      |      |       |        |      |
|    1 |    2 |    2 |    1 |    1 |    0 |    0 | 0.697 |  0.460 |    1 |
|    2 |    0 |    2 |    0 |    1 |    0 |    0 | 0.774 |  0.376 |    1 |
|    3 |    0 |    2 |    1 |    1 |    0 |    0 | 0.634 |  0.264 |    1 |
|    4 |    2 |    2 |    0 |    1 |    0 |    0 | 0.608 |  0.318 |    1 |
|    5 |    1 |    2 |    1 |    1 |    0 |    0 | 0.556 |  0.215 |    1 |
|    6 |    2 |    1 |    1 |    1 |    2 |    1 | 0.403 |  0.237 |    1 |
|    7 |    0 |    1 |    1 |    2 |    2 |    1 | 0.481 |  0.149 |    1 |
|    8 |    0 |    1 |    1 |    1 |    2 |    0 | 0.437 |  0.211 |    1 |
|    9 |    0 |    1 |    0 |    2 |    2 |    0 | 0.666 |  0.091 |    0 |
|   10 |    2 |    0 |    2 |    1 |    1 |    1 | 0.243 |  0.267 |    0 |
|   11 |    1 |    0 |    2 |    0 |    1 |    0 | 0.245 |  0.057 |    0 |
|   12 |    1 |    2 |    1 |    0 |    1 |    1 | 0.343 |  0.099 |    0 |
|   13 |    2 |    1 |    1 |    2 |    0 |    0 | 0.639 |  0.161 |    0 |
|   14 |    1 |    1 |    0 |    2 |    0 |    0 | 0.657 |  0.198 |    0 |
|   15 |    0 |    1 |    1 |    1 |    2 |    1 | 0.360 |  0.370 |    0 |
|   16 |    1 |    2 |    1 |    0 |    1 |    0 | 0.593 |  0.042 |    0 |
|   17 |    2 |    2 |    0 |    2 |    2 |    0 | 0.719 |  0.103 |    0 |

```python
X = df.iloc[:, :-1]  # 属性
y = df.iloc[:, -1]  # 标签
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3) #随机划分训练集和测试集
```

### 2. 定义节点

```python
class TreeNode:
    def __init__(self, tp=0, label=0, decide=0, compare=0):
        self.nxt = dict() # 儿子节点(属性值:节点)(中间节点特有)
        self.type = tp # 0->叶子, 1->中间节点
        self.label = label # 0->坏瓜, 1->好瓜
        self.decide = decide # 划分属性(中间节点特有)
        self.compare = compare # 判断值(连续节点特有)
```

### 3. 定义决策树模型

```python
class DecisionTree:
    def __init__(self):
        pass
    
    def __Ent(self, y):
        p = pd.value_counts(y) / len(y)
        return -np.sum(p * np.log2(p))
    
    def __choose(self, D, A):
        EntD = self.__Ent(D[-1])
        calc1 = lambda x : x.shape[0] / D[0].shape[0] * self.__Ent(x) # 计算信息熵
        calc2 = lambda x : x.shape[0] / D[0].shape[0] * np.log2(x.shape[0] / D[0].shape[0]) # 计算IV
        mp = dict()
        for attr in A:
            v = None  # 记录连续值判别的值
            if self.attr_type[attr]: # 连续值
                gain = 0
                iv = 0x3f3f3f3f
                for value in sorted(np.unique(D[0][:, attr])):
                    Dv1 = D[-1][D[0][:, attr] <= value]
                    Dv2 = D[-1][D[0][:, attr] > value]
                    temp = EntD - (calc1(Dv1) + calc1(Dv2))
                    if temp > gain:
                        gain = temp
                        iv = -calc2(Dv1)-calc2(Dv2)
                        v = value
                        # print("iv=", iv)
            else: # 离散值
                sm = 0
                iv = 1
                for value in np.unique(D[0][:, attr]):
                    Dv = D[-1][D[0][:, attr] == value]
                    sm += calc1(Dv)
                    iv += -calc2(Dv)
                gain = EntD - sm
            # print(gain, iv)
            gain_ratio = gain / iv
            mp[attr] = (gain, 2, v) # 去除gain_ratio的影响
        # print(mp)
        temp = sorted(list(mp.items()), key=lambda x : (-x[-1][0], -x[-1][1]))[0]
        return temp[0],temp[-1][-1] 
    
    def __generate(self, D, A):
        node = TreeNode()
        if np.unique(D[-1]).shape[0] <= 1:
            return TreeNode(tp=0, label=D[-1][0])
        if len(A) == 0 or np.unique(D[0][:, A], axis=0).shape[0] <= 1:
            return TreeNode(tp=0, label=np.argmax(np.bincount(D[1])))
        attr, v = self.__choose(D, A)
        # print(attr)
        node.decide = attr
        node.type = 1
        node.label = np.argmax(np.bincount([int(x) for x in D[1]]))
        if self.attr_type[attr]: # 连续值
            Dv = D[0][D[0][:, attr] <= v]
            # print(Dv)
            if Dv.shape[0] == 0:
                node.nxt[0] = TreeNode(tp=0, label=node.label)
            else:
                node.nxt[1] = self.__generate((Dv, D[1][D[0][:, attr] <= v]), [x for x in A if x != attr])
            Dv = D[0][D[0][:, attr] > v]
            # print(Dv)
            if Dv.shape[0] == 0:
                node.nxt[0] = TreeNode(tp=0, label=node.label)
            else:
                node.nxt[1] = self.__generate((Dv, D[1][D[0][:, attr] > v]), [x for x in A if x != attr])
        else: # 离散值
            for value in np.unique(D[0][:, attr]):
                Dv = D[0][D[0][:, attr] == value]
                if Dv.shape[0] == 0:
                    node.nxt[value] = TreeNode(tp=0, label=node.label)
                else:
                    node.nxt[value] = self.__generate((Dv, D[1][D[0][:, attr] == value]), [x for x in A if x != attr])
        return node
    
    def calc(self, X, y):  # 将数据通过决策树，得出结果，返回准确率
        n = X.shape[0]
        lst = []
        for i in range(X.shape[0]):
            x = X[i, :]
            node = self.root
            while node.type == 1:
                if self.attr_type[node.decide]:
                    if x[node.decide] <= node.compare:
                        node = node.nxt[0]
                    else:
                        node = node.nxt[1]
                else:
                    if x[node.decide] not in node.nxt.keys():
                        break
                    node = node.nxt[x[node.decide]]
            lst.append(node.label)
        ans = np.array(lst)
        acc = 1- np.sum(np.abs(ans - y)) / y.shape[0]
        return acc
    
    def fit(self, X, y):
        A = [i for i in range(X.values.shape[1])]
        self.attr_type = [X[label].dtype == float for label in X.columns] # 是否是连续值
        self.root = self.__generate((X.values, y.values), A)
        ans = self.calc(X.values, y.values)
        print(f"训练集准确率:{ans: .2f}")
    
    def predict(self, X, y):
        ans = self.calc(X.values, y.values)
        print(f"测试集准确率:{ans: .2f}")
```

### 4. 训练、测试

```python
model = DecisionTree()
model.fit(X_train, y_train)
model.predict(X_test, y_test)
```

训练集准确率: 1.00
测试集准确率: 0.67

### 5. 讨论

本题中由于数据量较少，所以建立的决策树模型可以无需使用gain_ratio，实验发现：在本数据集中使用gain_ratio效果还没有gain好。

### 6. 决策树

![new2.drawio](F:\University\001专业课\07大三上\机器学习基础\作业\assets\new2.drawio.svg)



## 4.5

本题目中仅有决策树模型中`__choose`函数与上题不同。下面是本题的`__choose`函数

```python
    def __choose(self, D, A):
        mp = {}
        for attr in A:
            X = D[0][:, attr:attr+1]
            y = D[1]
            v = None
            if self.attr_type[attr]:
                X_ =  X.copy()
                ans = -1
                for value in np.unique(X):
                    X_ = X < value
                    model = LogisticRegression()
                    model.fit(X_, y)
                    t = model.score(X_, y)
                    if t > ans:
                        ans = t
                        v = value
            else:
                model = LogisticRegression()
                # print(X, y)
                model.fit(X, y)
                ans = model.score(X, y)
            mp[attr] = (ans, v)
        temp = sorted(list(mp.items()), key=lambda x : -x[1][0])
        return temp[0][0], temp[0][1][1]
```

训练集准确率: 1.00
测试集准确率: 0.83

### 决策树

![new.drawio](F:\University\001专业课\07大三上\机器学习基础\作业\assets\new.drawio.svg)

## 4.7

**输入**：训练集$D={(\boldsymbol{x}_1, y_1),(\boldsymbol{x}_2, y_2),...,(\boldsymbol{x}_m, y_m)}$；属性集$A={a_1, a_2, ..., a_d}$

**过程**：函数$\text{TreeGenerate}(D, A)$

1. 生成根节点$\text{root}$；用于存储中间节点的队列$\text{queue}$；
2. 将$(\text{root}, D, A)$放入队列$\text{queue}$中；
3. $\bold{while}$ $\text{queue}$不为空 $\bold{do}$
4. ​    从$\text{queue}$中取出节点$\text{node}$、训练集$D$和属性集$A$；
5. ​    $\bold{if}$ $D$中样本全属于同一类别$C$ $\bold{then}$
6. ​        将$\text{node}$标记为$C$类叶节点； $\bold{continue}$
7. ​    $\bold{end}\ \bold{if}$    
8. ​    $\bold{if}$ $A = \varnothing$ $\bold{OR}$ $D$中样本在$A$上取值相同 $\bold{then}$
9. ​        将$\text{node}$标记为叶节点，其类型标记为$D$中样本数最多的类； $\bold{continue}$
10. ​    $\bold{end}\ \bold{if}$
11. ​    从$A$中选择最优划分属性$a_*$；
12. ​    $\bold{for}$ $a_*$的每一个值$a_*^v$ $\bold{do}$
13. ​        为$\text{node}$生成一个分支节点$\text{branch}$；令$D_v$表示$D$中在$a_*$上取值为$a_*^v$的样本子集；
14. ​        $\bold{if}$ $D_v$为空 $\bold{then}$
15. ​            将分支节点$\text{branch}$标记为叶节点，其类型标记为$D$中样本最多的类；
16. ​        $\bold{else}$
17. ​            将$(\text{branch}, D_v, A\backslash \{ a_* \})$放入队列$\text{queue}$中；
18. ​        $\bold{end}\ \bold{if}$
19. ​    $\bold{end}\ \bold{for}$
20. $\bold{end}\ \bold{while}$

**输出**：以$\text{root}$为根节点的一棵决策树



## 4.9

**答**：在含缺失值的样本中进行划分属性的选择，只需要将包含所有样本的集合$D$转化为只包含在属性$a$上非缺失值样本的集合$\tilde{D}$，并乘上权重即可。具体来说，计算基尼指数的公式如下：
$$
\text{Gini\_index}(D, a)=\rho \times \sum_{v=1}^{V} \tilde r_v \frac{|\tilde D^v|}{|\tilde D|}\text{Gini}(\tilde D^v)
$$
其中，
$$
\begin{align}
\text{Gini}(\tilde D) &= 1-\sum_{k=1}^{|\mathcal Y|}\tilde p_k^2, \\
\rho &= \frac {\sum_{\boldsymbol x \in\tilde  D} w_{\boldsymbol x}} {\sum_{\boldsymbol x \in D} w_{\boldsymbol x}}, \\
\tilde p_k &= \frac {\sum_{\boldsymbol x \in\tilde  D_k} w_{\boldsymbol x}} {\sum_{\boldsymbol x \in D} w_{\boldsymbol x}} (1 \leq k \leq |\mathcal Y |),\\
\tilde r_v &= \frac {\sum_{\boldsymbol x \in\tilde  D^v} w_{\boldsymbol x}} {\sum_{\boldsymbol x \in D} w_{\boldsymbol x}} (1\leq v \leq V),
\end{align}
$$
​                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
