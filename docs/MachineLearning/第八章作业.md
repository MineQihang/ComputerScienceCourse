# 《机器学习基础》第八章作业

## 8.1

使用简单投票法的集成学习预测错误，意味着少于半数的基学习器预测正确。那么，集成学习的错误率就是少于半数的基学习器预测正确的概率。这等价于抛$T$次硬币，有少于$\lfloor T/2 \rfloor$的硬币朝上的概率（这里硬币朝上的概率为$1-\epsilon$，也就是一个基学习器预测正确的概率）。预测正确的基学习器数量为
$$
g(x) = \sum_{i=0}^{T} \mathbb I (h_i(x)=f(x))
$$
那么$g(x) \leq \lfloor T / 2 \rfloor$的概率为
$$
P(g(x) \leq \lfloor T/2 \rfloor)=\sum_{k=0}^{\lfloor T/2 \rfloor}\binom{T}{k}(1-\epsilon)^k\epsilon^{T-k}
$$
这就是式(8.3)的左半部分$P(H(x)\neq f(x))$，由题中给出的定义可得$\lfloor T/2 \rfloor= (\epsilon - \delta)T$，于是$\delta = \epsilon - \lfloor T/2 \rfloor/T \ge \epsilon - 1/2$，由Hoeffding不等式得
$$
P(H(x)\neq f(x)) \leq \exp\left( -2 \left( \epsilon - \frac12 \right)^2 T \right) = \exp\left( -\frac12 T \left( 1 - 2\epsilon \right)^2 \right)
$$
式(8.3)得证。



## 8.2

 我们需要最小化的就是$\mathbb E_{x\in D}[\ell(-f(x) H(x))]$，考虑对$H(x)$进行求偏导，得到
$$
-P(f(x)=1|x) \ell(-H(x)) +P(f(x)=-1|x) \ell(H(x))
$$
令其为0得到
$$
\frac{P(f(x)=-1|x)}{P(f(x)=1|x)} = \frac{\ell(H(x))}{\ell(-H(x))}
$$
当$P(f(x)=-1|x) > P(f(x)=1|x)$时，$\ell(H(x)) > \ell(-H(x))$，由于$\ell(-f(x)H(x))$关于$H(x)$递减，因此
$$
H(x) < -H(x)
$$
所以$H(x)<0$。同理，当$P(f(x)=-1|x) < P(f(x)=1|x)$时，$H(x)>0$。

综上：$\text{sign}(H(x))=\arg \max_{y\in (-1, 1)}P(f(x)=y|x)$，这意味着$\text{sign}(H(x))$达到了贝叶斯最优错误率。这说明此损失函数是分类任务原本0/1损失函数得一致性替代函数。



## 8.3

使用`sklean.ensemble`中的`AdaBoostClassifier`实现即可（默认采用决策树模型作为基学习器），代码如下：

```python
from sklearn.ensemble import AdaBoostClassifier
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np

# 加载数据
df = pd.read_csv("./3.0a.csv", index_col=0)
X = df.iloc[:, :2].values
y = df.iloc[:, -1].values

# 模型训练与预测
model = AdaBoostClassifier(n_estimators=1, random_state=0)
model.fit(X, y)
pred = model.predict(X)
print("错误分类数量:", np.sum(np.abs(pred - y)))

# 画图
plt.scatter(X[:, 0], X[:, 1], c=pred)
```

基学习器数量为1时：

![image-20221124170806049](https://qihang-1306873228.cos.ap-chongqing.myqcloud.com/imgs/image-20221124170806049.png)

基学习器数量为2时：

![image-20221124170846400](https://qihang-1306873228.cos.ap-chongqing.myqcloud.com/imgs/image-20221124170846400.png)

基学习器数量超过3时：

![image-20221124170927442](https://qihang-1306873228.cos.ap-chongqing.myqcloud.com/imgs/image-20221124170927442.png)

可以发现，当基学习器变多的时候分类效果得到了提升，并且很稳定！



## 8.5

使用`sklean.ensemble`中的`BaggingClassifier`实现即可（默认采用决策树模型作为基学习器），代码如下：

```python
from sklearn.ensemble import BaggingClassifier
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np

# 加载数据
df = pd.read_csv("./3.0a.csv", index_col=0)
X = df.iloc[:, :2].values
y = df.iloc[:, -1].values

# 模型训练与预测
model = BaggingClassifier(n_estimators=1, random_state=0)
model.fit(X, y)
pred = model.predict(X)
print(np.sum(np.abs(pred - y)))

# 画图
plt.scatter(X[:, 0], X[:, 1], c=pred)
```

基学习器数量为1时：

![image-20221124170957051](https://qihang-1306873228.cos.ap-chongqing.myqcloud.com/imgs/image-20221124170957051.png)

基学习器数量为2，3，4时：

![image-20221124171014036](https://qihang-1306873228.cos.ap-chongqing.myqcloud.com/imgs/image-20221124171014036.png)

基学习器数量为7时：

![image-20221124171147844](https://qihang-1306873228.cos.ap-chongqing.myqcloud.com/imgs/image-20221124171147844.png)

可以发现，当基学习器变多的时候分类效果不一定得到提升，在分类器数量较小时，变化比较不稳定。



## 8.6

Bagging方法使用自助采样，简单高效，但考虑到“误差-分歧分解”，Bagging方法中个体学习器的多样性较低。朴素贝叶斯属于稳定基学习器，对数据样本的扰动不敏感，对这类基学习器进行集成往往需要使用输入属性扰动等其他扰动，而Bagging方法做不到，因此，Bagging方法通常很难提升朴素贝叶斯分类器的性能。

