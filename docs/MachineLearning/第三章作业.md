# 《机器学习》第三章作业

## 3.1

答：（1）该模型中$b$与输入没有关系时或模型函数一定过原点时；（2）该模型只用于差分分析时，因为假设取两实例$\boldsymbol{x}_i$和$\boldsymbol{x}_j(i\neq j)$，$f(\boldsymbol{x}_i) - f(\boldsymbol{x}_j ) = \boldsymbol{w}^\text{T}(\boldsymbol{x}_i - \boldsymbol{x}_j)$是与$b$无关的。

## 3.3

### 1.准备数据

```python
import numpy as np
from sklearn.model_selection import train_test_split

data = np.array([[0.697, 0.460, 1],
        [0.774, 0.376, 1],
        [0.634, 0.264, 1],
        [0.608, 0.318, 1],
        [0.556, 0.215, 1],
        [0.403, 0.237, 1],
        [0.481, 0.149, 1],
        [0.437, 0.211, 1],
        [0.666, 0.091, 0],
        [0.243, 0.267, 0],
        [0.245, 0.057, 0],
        [0.343, 0.099, 0],
        [0.639, 0.161, 0],
        [0.657, 0.198, 0],
        [0.360, 0.370, 0],
        [0.593, 0.042, 0],
        [0.719, 0.103, 0]])  # 西瓜数据集3.0alpha
X = data[:, 0:2]  # 属性
y = data[:, -1]  # 标签
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3) #随机划分训练集和测试集
```

### 2.定义模型

```python
class LogisticRegression:
    def __int__(self):
        pass
    
    def fit(self, X_, y_):
        # 数据调整为x_bar和\beta, 便于操作
        n, m = X_.shape
        m += 1
        X = np.append(X_, np.ones(n).reshape((n, 1)), axis=1).T
        y = y_.copy()
        beta = np.random.random(size=(m, 1))
        # 牛顿迭代法
        epoch = 20
        for i in range(epoch):
            mul = beta.T @ X
            p1 = np.exp(mul) / (1 + np.exp(mul))  # 计算p1的公式
            fd = -np.sum(X * (y - p1), axis=1)  # 计算牛顿迭代中的一阶导数
            sd = np.zeros((m, m))  # 计算牛顿迭代中的二阶导数
            for i in range(n):
                sd += X[:, i:i+1] @ X[:, i:i+1].T * (p1[0, i] * (1 - p1[0, i]))
            beta = beta - (np.linalg.inv(sd) @ fd).reshape((m, 1))  # 一次迭代
        # 训练结果分析
        ans = (beta.T @ X) > 1
        self.beta = beta
        print("训练集准确率: ", 1 - np.sum(np.abs(ans - y)) / n)

    def predict(self, X_, y_):
        # 测试结果分析
        n = X_.shape[0]
        X = np.append(X_, np.ones(n).reshape((n, 1)), axis=1).T
        y = y_.copy()
        ans = (self.beta.T @ X) > 1
        print("测试集准确率: ", 1 - np.sum(np.abs(ans - y)) / n)

```

### 3.使用模型进行训练和预测

```python
model = LogisticRegression()
model.fit(X_train, y_train)
model.predict(X_test, y_test)
```

训练集准确率:  0.7272727272727273
测试集准确率:  0.8333333333333334

## 3.5

### 1.准备数据

同3.3

### 2.定义模型计算𝜔

```python
class LDA:
    def __init__(self):
        pass
    
    def fit(self, X, y):
        X0 = X[y == 0]  # 坏瓜
        X1 = X[y == 1]  # 好瓜
        mu0 = np.mean(X0, axis=0).reshape((-1, 1))  # 坏瓜均值
        mu1 = np.mean(X1, axis=0).reshape((-1, 1))  # 好瓜均值
        cov0 = np.cov(X0, rowvar=False)  # 坏瓜协方差
        cov1 = np.cov(X1, rowvar=False)  # 好瓜协方差
        S_w = np.mat(cov0 + cov1)  # S_w
        U,Sigma,V = np.linalg.svd(S_w,full_matrices=False)  # 奇异值分解
        self.omega = V.T @ np.linalg.inv(np.diag(Sigma)) @ U.T @ (mu0 - mu1)  # /omega
        # 输出结果
        ans = self.omega.T @ X.T < 0
        acc = 1 - np.sum(np.abs(ans - y)) / y.shape[0]
        print("训练集准确率:", acc)
    
    def predict(self, X, y):
        # 输出结果
        ans = self.omega.T @ X.T < 0
        acc = 1- np.sum(np.abs(ans - y)) / y.shape[0]
        print("测试集准确率:", acc)

```

### 3.使用模型进行训练和预测

```python
model = LDA()
model.fit(X_train, y_train)
model.predict(X_test, y_test)
```

训练集准确率: 0.9090909090909091
测试集准确率: 0.6666666666666667

## 3.7

答：给出汉明距离意义下理论最优的ECOC二元码：

|       | $f_1$ | $f_2$ | $f_3$ | $f_4$ | $f_5$ | $f_6$ | $f_7$ | $f_8$ | $f_9$  |
| ----- | ----- | ----- | ----- | ---- | ---- | ------- | ---- | ---- | ---- |
| $C_1$ | +1 | -1 | -1 | -1 | +1 | +1 | +1 | +1 | -1 |
| $C_2$ | -1 | +1 | -1 | -1 | +1 | -1 | -1 | +1 | -1 |
| $C_3$ | -1 | -1 | +1 | -1 | -1 | +1 | -1 | +1 | -1 |
| $C_4$ | -1 | -1 | -1 | +1 | -1 | -1 | +1 | +1 | -1 |

证明如下：

注意到性质：若两个分类器$f_i$和$f_j(i \neq j)$对样例的分类互为反码[如$(-1,-1,1,1)$和$(1,1,-1,-1)$]，由于很多算法对待0-1分类是对称的（即如果将0-1类互换，最终训练出的模型是一样的）。这样一对分类器实际上可以简化为一个分类器，因此可以计算出在不出现互为反码的情况下，最优的ECOC编码长度为
$$
\binom{4}{1} + \frac{\binom{4}{2}}{2}=7
$$
由于可以向编码中加入冗余，即可以在最短编码长度的情况下加入互为反码的一对分类器，因此只要保证编码长度为$7+2n(n为自然数)$即可保证得出的ECOC编码是最优的。

综上，由于$9=7+2\times 1$（$f_8$和$f_9$是加入的冗余分类器），因此上述给出的ECOC二元码是理论最优的。

## 3.9

答：对OvR、MvM 来说，由于**对每个类进行了相同的处理**，其拆解出的二分类任务中类别不平衡的影响会相互抵消，因此通常不需专门处理。
