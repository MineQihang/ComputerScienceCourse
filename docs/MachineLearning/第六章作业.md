# 《机器学习》第六章作业

## 6.1

**答**：设超平面上任意一点为$\boldsymbol{y}$，满足$\boldsymbol{w}^{\text T} \boldsymbol{y} + b = 0$，那么$\boldsymbol{x}$与$\boldsymbol{y}$的距离为$|\boldsymbol{x} - \boldsymbol{y}|$。由几何关系可知，只有当$\boldsymbol{x} - \boldsymbol{y}$与超平面垂直时，才能满足$\boldsymbol{x}$与超平面的距离为$|\boldsymbol{x} - \boldsymbol{y}|$，又因为$\boldsymbol{w}$与超平面垂直，因此
$$
\boldsymbol{x} - \boldsymbol{y} = \pm |\boldsymbol{x} - \boldsymbol{y}| \frac{\boldsymbol{w}}{||\boldsymbol{w}||}
$$
两边同时左乘$\boldsymbol w^{\text T}$可得
$$
\boldsymbol w^{\text T} \boldsymbol{x} - \boldsymbol w^{\text T} \boldsymbol{y} &= \pm |\boldsymbol{x} - \boldsymbol{y}|  \frac{\boldsymbol w^{\text T}\boldsymbol{w}}{||\boldsymbol{w}||} \\
\boldsymbol w^{\text T} \boldsymbol{x} + b &= \pm |\boldsymbol{x} - \boldsymbol{y}| \times ||\boldsymbol w||
$$
即
$$
r = |\boldsymbol{x} - \boldsymbol{y}| = \frac{|\boldsymbol w^{\text T} \boldsymbol{x} + b|}{||\boldsymbol w||}
$$

## 6.2

### 1. 搭建环境


```python
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from sklearn import svm
```

### 2. 导入数据


```python
df = pd.read_csv("./3.0a.csv", index_col=0)
X = df.iloc[:, :2].values
y = df.iloc[:, -1].values
```

### 3. 使用线性核训练一个SVM


```python
model = svm.SVC(C=1000, kernel='linear')  # C高一些，对误差的容忍度低
model.fit(X, y)
model.score(X, y)
```


    0.8235294117647058


```python
# 绘制支持向量
idx = model.support_
plt.scatter(X[:, 0], X[:, 1], c=y, s=30)
plt.scatter(X[idx][:, 0], X[idx][:, 1], c=y[idx], edgecolors='r', s=100)
plt.show()
```


![output_6_0](F:\University\001专业课\07大三上\机器学习基础\作业\assets\output_6_0.png)


###  4. 使用高斯核训练一个SVM


```python
model = svm.SVC(C=1000, kernel='rbf')  # C高一些，对误差的容忍度低
model.fit(X, y)
model.score(X, y)
```

```
1.0
```


```python
# 绘制支持向量
idx = model.support_
plt.scatter(X[:, 0], X[:, 1], c=y, s=30)
plt.scatter(X[idx][:, 0], X[idx][:, 1], c=y[idx], edgecolors='r', s=100)
plt.show()
```

![output_9_0](F:\University\001专业课\07大三上\机器学习基础\作业\assets\output_9_0.png)

### 5. 对比分析

在对误差容忍度比较高的情况下，高斯核SVM可以完全拟合数据，而且支持向量也比较少。但由于数据线性不可分，线性核SVM不论设置多高的容忍度仍不能很好地区分数据，且支持向量也较多。

## 6.3

### 1. 搭建环境


```python
import numpy as np
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris, load_breast_cancer
from sklearn.svm import SVC  # 支持向量机
from sklearn.neural_network import MLPClassifier  # BP神经网络
from sklearn.tree import DecisionTreeClassifier  # 决策树
```

### 2. 构建数据集


```python
X_iris, y_iris = load_iris(return_X_y=True)  # 鸢尾花数据集
X_bc, y_bc = load_breast_cancer(return_X_y=True)  # 乳腺癌数据集
# 划分训练集和测试集
X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=0)
X_bc_train, X_bc_test, y_bc_train, y_bc_test = train_test_split(X_bc, y_bc, test_size=0.2, random_state=0)
```

### 3. 定义各种模型函数


```python
def svm(X_train, X_test, y_train, y_test, kernel="linear"):
    model = SVC(C=1, kernel=kernel)
    model.fit(X_train, y_train)
    print(f"[SVM with {kernel} kernel]Accuracy: {model.score(X_test, y_test)*100: .2f}%")
```


```python
def mlp(X_train, X_test, y_train, y_test, activation="logistic"):
    model = MLPClassifier(max_iter=1000, activation=activation, random_state=0)
    model.fit(X_train, y_train)
    print(f"[BP Neural Network with {activation}]Accuracy: {model.score(X_test, y_test)*100: .2f}%")
```


```python
def dt(X_train, X_test, y_train, y_test, criterion="entropy"):
    model = DecisionTreeClassifier(criterion=criterion, random_state=0)
    model.fit(X_train, y_train)
    print(f"[Decision Tree with {criterion}]Accuracy: {model.score(X_test, y_test)*100: .2f}%")
```

### 4. 在鸢尾花数据集上测试


```python
svm(X_iris_train, X_iris_test, y_iris_train, y_iris_test, kernel="linear")
svm(X_iris_train, X_iris_test, y_iris_train, y_iris_test, kernel="rbf")
mlp(X_iris_train, X_iris_test, y_iris_train, y_iris_test, activation="logistic")
mlp(X_iris_train, X_iris_test, y_iris_train, y_iris_test, activation="relu")
dt(X_iris_train, X_iris_test, y_iris_train, y_iris_test, criterion="entropy")
dt(X_iris_train, X_iris_test, y_iris_train, y_iris_test, criterion="gini")
```

    [SVM with linear kernel]Accuracy:  100.00%
    [SVM with rbf kernel]Accuracy:  100.00%
    [BP Neural Network with logistic]Accuracy:  100.00%
    [BP Neural Network with relu]Accuracy:  100.00%
    [Decision Tree with entropy]Accuracy:  100.00%
    [Decision Tree with gini]Accuracy:  100.00%


### 5. 在乳腺癌数据集上测试


```python
svm(X_bc_train, X_bc_test, y_bc_train, y_bc_test, kernel="linear")
svm(X_bc_train, X_bc_test, y_bc_train, y_bc_test, kernel="rbf")
mlp(X_bc_train, X_bc_test, y_bc_train, y_bc_test, activation="logistic")
mlp(X_bc_train, X_bc_test, y_bc_train, y_bc_test, activation="relu")
dt(X_bc_train, X_bc_test, y_bc_train, y_bc_test, criterion="entropy")
dt(X_bc_train, X_bc_test, y_bc_train, y_bc_test, criterion="gini")
```

    [SVM with linear kernel]Accuracy:  95.61%
    [SVM with rbf kernel]Accuracy:  92.98%
    [BP Neural Network with logistic]Accuracy:  91.23%
    [BP Neural Network with relu]Accuracy:  94.74%
    [Decision Tree with entropy]Accuracy:  92.98%
    [Decision Tree with gini]Accuracy:  91.23%

### 6. 分析

针对不同的数据集特征可以选择不同的算法。通过针对不同数据集对不同算法的参数进行调节，很多模型都能很好地拟合数据。

## 6.4

**答**：线性判别分析是将$n$维的数据投影到$n-1$维上的一个超平面后再来分类，可以支持多分类；而线性核的SVM是在$n$维的数据中寻找一个$n-1$的超平面来分隔数据，是解决的二分类问题。因此，若需线性判别分析与线性核等价，一定要满足都是在**解决二分类问题**上（不考虑使用OvR等使SVM解决多分类）。

由于线性判别分析关键在找到映射平面，线性核的SVM关键在找分类超平面。考虑两者的关系，可以想到若**两平面垂直**时，如果线性判别分析能达成分类任务，那么在映射平面的两类之间取一条直线，并以这条直线和映射平面的法向量相交做一个平面，一定可以分隔样本。而这个样本就可以作为线性核SVM的分类超平面。

综上所述，线性判别分析与线性核支持向量机满足以下两个条件时等价：

- 解决的问题是二分类问题；
- 线性判别分析的映射超平面和线性核支持向量机的分类超平面垂直。

## 6.6

**答**：对于硬间隔支持向量机，若噪声样本出现在支持向量中，会使决策超平面变化，间隔变小，使得SVM的泛化能力变弱、检测精度降低；对于软间隔支持向量机，噪声样本的出现会使目标函数中的损失项显著提高，使SVM决策受到影响。因此SVM对噪声敏感。
