# 《机器学习基础》第九章作业

## 9.1

**（1）首先证明闵可夫斯基距离满足非负性，同一性和对称性。**

由于
$$
\text{dist}_\text{mk} (x_i, x_j) = \left( \sum_{u=1}^n |x_{iu}-x_{ju}|^p \right)^\frac 1p \ge 0
$$
因此，闵可夫斯基距离满足非负性。当且仅当$x_i = x_j$时，$\text{dist}_\text{mk} (x_i, x_j)=0$，因此闵可夫斯基距离满足同一性。由于绝对值中减数和被减数可以调换位置，所以$\text{dist}_\text{mk} (x_i, x_j) = \text{dist}_\text{mk} (x_j, x_i)$，因此闵可夫斯基距离满足对称性。

**（2）下面证明当$p\ge 1$时，闵可夫斯基距离满足直递性。**

显然$p=1$时，可由三角不等式证明。因此下面$p>1$。

令$a_i=|x_{iu}-x_{ku}|,b_i=|x_{ku}-x_{ju}|$，那么要证$\text{dist}_\text{mk} (x_i, x_j) \leq \text{dist}_\text{mk} (x_i, x_k) + \text{dist}_\text{mk} (x_k, x_j)$，即证
$$
\left(\sum_{i=1}^{n}|a_i+b_i|^p\right)^\frac 1p \leq \left(\sum_{i=1}^{n}a_i^p \right)^\frac 1p + \left(\sum_{i=1}^{n}b_i^p \right)^\frac 1p
$$
由于
$$
\begin{align}
\left(\sum_{i=1}^{n}|a_i+b_i|^p\right)^\frac 1p &\leq \sum_{i=1}^{n}|a_i+b_i|^p \\
&= \sum_{i=1}^{n}|a_i+b_i||a_i+b_i|^{p-1} \\
&\leq  \sum_{i=1}^{n}(|a_i|+|b_i|)|a_i+b_i|^{p-1} \\
&= \sum_{i=1}^{n}(|a_i||a_i+b_i|^{p-1}+|b_i||a_i+b_i|^{p-1})
\end{align}
$$
由赫尔德不等式$\sum_i^na_ib_i \leq (\sum_{i=1}^n a_i^p)^\frac1p(\sum_{i=1}^nb_i^q)^\frac1q \ \ (p, q > 1) $可得
$$
\begin{align}
\sum_{i=1}^{n}(|a_i||a_i+b_i|^{p-1}+|b_i||a_i+b_i|^{p-1}) &\leq \left(\sum_{i=1}^{n}|a_i|^p \right)^{\frac1p} \left(\sum_{i=1}^{n}|a_i+b_i|^{(p-1)q}\right)^{\frac1q} + \left(\sum_{i=1}^{n}|b_i|^p\right)^{\frac1p} \left(\sum_{i=1}^{n}|a_i+b_i|^{(p-1)q}\right)^{\frac1q} \\
&= \left(\left(\sum_{i=1}^{n}|a_i|^p\right)^{\frac1p} + \left(\sum_{i=1}^{n}|b_i|^p\right)^{\frac1p}  \right) \left(\sum_{i=1}^{n}|a_i+b_i|^{p}\right)^{\frac1q} \\
&\leq \left(\sum_{i=1}^{n}|a_i|^p\right)^{\frac1p} + \left(\sum_{i=1}^{n}|b_i|^p\right)^{\frac1p}
\end{align}
$$
其中$q=\frac{p}{p-1}$，于是原不等式成立，因此，当$p \ge 1$时闵可夫斯基距离满足直递性。

**（3）下面证明当$0 \leq p < 1$时，闵可夫斯基距离不满足直递性。**

和（2）类似的，只需证明下式即可证明原命题。
$$
\left(\sum_{i=1}^{n}|a_i+b_i|^p\right)^\frac 1p \ge \left(\sum_{i=1}^{n}a_i^p \right)^\frac 1p + \left(\sum_{i=1}^{n}b_i^p \right)^\frac 1p
$$
由于
$$
\begin{align}
\sum_{i=1}^{n}|a_i+b_i|^p &= \sum_{i=1}^{n}|t\frac{a_i}{t}+(1-t)\frac{b_i}{1-t}|^p \\
&\ge t^{1-p}\sum_{i=1}^{n}|a_i|^p+(1-t)^{1-p}\sum_{i=1}^{n}|b_i|^p
\end{align}
$$
其中
$$
t=\frac{\left(\sum_{i=1}^{n}a_i^p \right)^\frac 1p}{\left(\sum_{i=1}^{n}a_i^p \right)^\frac 1p + \left(\sum_{i=1}^{n}b_i^p \right)^\frac 1p}
$$
于是有
$$
\begin{align}
t^{1-p}\sum_{i=1}^{n}|a_i|^p+(1-t)^{1-p}\sum_{i=1}^{n}|b_i|^p &= \left(\left(\sum_{i=1}^{n}a_i^p \right)^\frac 1p + \left(\sum_{i=1}^{n}b_i^p \right)^\frac 1p\right)^p
\end{align}
$$
综上即可证明原不等式成立，闵可夫斯基距离不满足直递性。

**（4）下面证明$p$趋向无穷大时，闵可夫斯基距离等于对应分量的最大绝对距离。**

闵可夫斯基距离公式可以转换如下：
$$
\text{dist}_\text{mk} (x_i, x_j) = \left( \sum_{u=1}^n |x_{iu}-x_{ju}|^p \right)^\frac 1p  = \max_u|x_{iu}-x_{ju}| \left(  \sum_{u=1}^n \left(\frac{|x_{iu}-x_{ju}|}{\max_u|x_{iu}-x_{ju}|}\right)^p \right)^\frac 1p
$$
令
$$
t_u = \frac{|x_{iu}-x_{ju}|}{\max_u|x_{iu}-x_{ju}|}\leq 1
$$
当$p \rarr +\infin$时
$$
\left(  \sum_{u=1}^n t_u^p \right)^\frac 1p \rarr1
$$
因此
$$
\text{dist}_\text{mk} (x_i, x_j) \rarr \max_u|x_{iu}-x_{ju}|
$$
综上得证。



## 9.2

显然，豪斯多夫距离满足非负性和对称性，这里不过多证明。

（1）下面证明豪斯多夫距离满足同一性。

①当$X=Z$时，$\text{dist}_\text{h}(X, Z) = \text{dist}_\text{h}(Z, X) = 0$，因此$\text{dist}_\text{H}(X, Z) = 0$。

②当$\text{dist}_\text{H}(X, Z) = 0$时，$\text{dist}_\text{h}(X, Z) = \text{dist}_\text{h}(Z, X) = 0$，这说明任意$x\in X$，存在$z\in Z$使得距离为0，因此$X \subset Z$，同时也说明任意$z\in Z$，存在$x\in X$使得距离为0，因此$Z \subset X$，所以$X=Z$。

综上得证。

（2）下面证明豪斯多夫距离满足直递性。

欲证
$$
\text{dist}_\text{H}(X, Z) \leq \text{dist}_\text{H}(X, Y) + \text{dist}_\text{H}(Y, Z)
$$

由于
$$
\text{dist}_\text{H}(X, Z) = \max(\text{dist}_\text{h}(X, Z), \text{dist}_\text{h}(Z, X))
$$
因此，可以转化为求证
$$
\text{dist}_\text{h}(X, Z) \leq \text{dist}_\text{h}(X, Y) + \text{dist}_\text{h}(Y, Z)
$$
由于
$$
\begin{align}
\min_{z\in Z} ||x - z||_2 &= \min_{z \in Z} ||x + y+y-z||_2, \forall y\in Y \\
&\leq  \min_{z \in Z} (||x + y||_2+||y-z||_2), \forall y\in Y \\
&= ||x+y||_2 + \min_{z \in Z} ||y-z||_2, \forall y\in Y \\
&\leq ||x+y||_2 + \max_{y \in Y}\min_{z \in Z} ||y-z||_2, \forall y\in Y \\
&= ||x+y||_2 + \text{dist}_\text{h}(Y, Z), \forall y\in Y \\ 
&\leq \min_{y \in Y} ||x+y||_2 + \text{dist}_\text{h}(Y, Z)
\end{align}
$$
对不等式左右两边取$\max_{x\in X}$得到
$$
\max_{x\in X}\min_{z\in Z} ||x - z||_2 \leq \max_{x\in X}\min_{y \in Y} ||x+y||_2 + \text{dist}_\text{h}(Y, Z)
$$
即
$$
\text{dist}_\text{h}(X, Z) \leq \text{dist}_\text{h}(X, Y) + \text{dist}_\text{h}(Y, Z)
$$
综上得证。




## 9.3

不一定能。由于求解式(9.24)的最优解是一个NP-hard问题，k均值算法只是基于贪心思想通过迭代进行近似求解得到的局部最优解，不一定能得到式子的全局最优解。



## 9.4

### 1 导入库
```python
from sklearn.cluster import KMeans
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
```

### 2 导入数据集
```python
df = pd.read_csv("./4.0.csv", header=None)
X = df.values
```

### 3 测试不同k值的影响
> 选定数据集中前k个点作为初始中心点
```python
for k in range(2, 5):
    model = KMeans(k, init=[X[i, :] for i in range(k)], n_init=1)
    model.fit(X)
    ans = model.predict(X)
    print(model.cluster_centers_)
    plt.title(f"KMeans [k={k}]")
    plt.scatter(X[:, 0], X[:, 1], c=ans)
    plt.show()
```

```
[[0.62371429 0.37435714]
 [0.44675    0.1875    ]]
```

![image-20221201101930648](https://qihang-1306873228.cos.ap-chongqing.myqcloud.com/imgs/image-20221201101930648.png)

```
[[0.471      0.39928571]
 [0.68138462 0.28446154]
 [0.3725     0.1748    ]]
```

![image-20221201101937755](https://qihang-1306873228.cos.ap-chongqing.myqcloud.com/imgs/image-20221201101937755.png)

```
[[0.67625    0.4665    ]
 [0.6896     0.3072    ]
 [0.61585714 0.137     ]
 [0.38685714 0.27714286]]
```

![image-20221201101941928](https://qihang-1306873228.cos.ap-chongqing.myqcloud.com/imgs/image-20221201101941928.png)



### 4 测试不同初始中心点的影响

> 选定k=3

```python
Xs = [[[0, 0], [0, 0], [0, 0]],  # 全都在一个点
      [[0.5, 0.1], [0.5, 0.25], [0.5, 0.4]],  # y轴分散
      [[0.3, 0.3], [0.5, 0.3], [0.7, 0.3]]]  # x轴分散

for centers in Xs:
    model = KMeans(3, init=centers, n_init=1)
    model.fit(X)
    ans = model.predict(X)
    print(model.cluster_centers_)
    # plt.title(f"KMeans [k={k}]")
    plt.scatter(X[:, 0], X[:, 1], c=ans)
    plt.show()
```

```
[[0.36136364 0.21709091]
 [0.60845455 0.41336364]
 [0.6515     0.16325   ]]
```

![image-20221201102105809](https://qihang-1306873228.cos.ap-chongqing.myqcloud.com/imgs/image-20221201102105809.png)

```
[[0.63255556 0.16166667]
 [0.33455556 0.21411111]
 [0.598      0.40491667]]
```

![](https://qihang-1306873228.cos.ap-chongqing.myqcloud.com/imgs/image-20221201102115855.png)

```
[[0.348      0.18955556]
 [0.471      0.39928571]
 [0.67507143 0.26714286]]
```

![image-20221201102142375](https://qihang-1306873228.cos.ap-chongqing.myqcloud.com/imgs/image-20221201102142375.png)

### 5 分析

1. 对比不同k值得到的结果可以看出，不是k越大越好，而是需要根据经验去确定簇的数量，这样才能达到不错的聚类效果。
2. 对比不同初始值的结果可以看出，初始中心点也很大程度上影响到了聚类的效果，选定的三组初始中心点各有特征，可以看出在选定初始中心点在x轴上分散分布时，效果比较不错，说明数据集在x轴方向上有比较好的可分性。由于KMeans基于贪心算法，只能得到局部最优解，因此根据经验选择符合数据集特征的初始中心点对聚类效果有显著影响。



## 9.5

（1）连接性。设$x_k$为集合$X$中的核心对象，设$x_i \in X, x_j \in X$。由于集合$X$是由$x_k$密度可达的所有样本构成的，那么$x_i,x_j$均由$x_k$密度可达，因此$x_i$与$x_j$密度相连。由此证明连接性。

（2）最大性。设$x_k$为集合$X$中的核心对象，设$x_i \in C$，那么$x_i$由$x_k$密度可达。如果$x_j$由$x_i$密度可达，由于密度可达关系满足直递性，$x_j$由$x_k$密度可达，因此$x_j\in C$。由此证明最大性。



