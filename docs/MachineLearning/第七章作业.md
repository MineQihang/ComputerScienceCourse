# 《机器学习基础》第七章作业

## 7.1

**答：**使用极大似然法得到的类条件概率估计表达式为
$$
P(x_i=t_{ij}|c) = \frac{|D_{c,x_i, t_{ij}}|}{|D_c|}
$$
其中，$|D_c|$表示训练集$D$中$c$类样本的数量，$|D_{c, x_i, t_{ij}}|$表示训练集中$c$类元素属性$x_i$为$t_{ij}$的数量的元素。由此根据西瓜数据集即可得出类条件概率如下：

| 属性        | 属性取值       | 好瓜($c_0$) | 坏瓜($c_1$) |
| ----------- | -------------- | ----------- | ----------- |
| 色泽($x_0$) | 青绿($t_{00}$) | $\frac38$   | $\frac13$   |
|             | 乌黑($t_{01}$) | $\frac12$   | $\frac29$   |
|             | 浅白($t_{02}$) | $\frac18$   | $\frac49$   |
| 根蒂($x_1$) | 蜷缩($t_{10}$) | $\frac58$   | $\frac13$   |
|             | 稍蜷($t_{11}$) | $\frac38$   | $\frac49$   |
|             | 硬挺($t_{12}$) | $0$         | $\frac29$   |
| 敲声($x_2$) | 浊响($t_{20}$) | $\frac34$   | $\frac49$   |
|             | 沉闷($t_{21}$) | $\frac14$   | $\frac13$   |
|             | 清脆($t_{22}$) | $0$         | $\frac29$   |



## 7.3

### 1. 环境及数据准备
```python
import numpy as np
import pandas as pd
import scipy.stats as st

df = pd.read_csv('./watermelon3.csv', index_col=0)  # 西瓜数据集3.0
X = np.array(df.iloc[:, :-1])  # 属性
y = np.array(df.iloc[:, -1])[None].T  # 标签
```

### 2. 定义模型
```python
class NaiveBayes:
    def __init__(self) -> None:
        pass
        
    def get_prob(self, X, x, x_i, N):  # 拉普拉斯修正
        return ((X[:, x] == x_i).sum() + 1) / (X.shape[0] + N)

    def get_dis_prob(self, X, y):  # 离散概率
        p = []
        for i, c in enumerate(self.label):
            temp = X[(y == c).ravel()]
            lst = []
            for x in range(temp.shape[1]):
                dct = {}
                uni = np.unique(X[:, x])
                for x_i in uni:
                    dct[x_i] = self.get_prob(temp, x, x_i, uni.shape[0])
                lst.insert(x, dct)
            p.insert(i, lst)
        return p

    def get_con_prob(self, X, x, x_i):  # 连续概率
        return st.norm.pdf(x_i, loc=X[:, x].mean(), scale=X[:, x].std())

    def train(self, X, y):
        self.X, self.y = X, y
        self.con_col = [i for i in range(X.shape[1]) if type(X[0][i]) == float]  # 连续值所在位置
        dis_X = np.delete(X, self.con_col, axis=1)  # 提取离散值
        con_X = X[:, self.con_col]  # 提取连续值
        self.label = np.unique(y)  # 将label的类型取出来
        self.dis_p = self.get_dis_prob(dis_X, y)
        self.P_c = ((self.label == y).sum(axis=0) + 1) / (df.shape[0] + self.label.shape[0])

    def predict(self, X_pre):
        y_pre = np.ones(X_pre.shape[0], dtype='object')
        for j, X in enumerate(X_pre):
            maxn = 0
            res_c = 0
            for i, c in enumerate(self.label):
                res = 1
                temp_X = self.X[(self.y == c).ravel()]
                a = 0
                for x in range(X.shape[0]):
                    if x in self.con_col:  # 连续
                        res *= self.get_con_prob(temp_X, x, X[x])
                    else:  # 离散
                        res *= (self.dis_p[i][a][X[x]])
                        a += 1
                res *= self.P_c[i]
                if res > maxn:
                    maxn = res
                    res_c = c
            y_pre[j] = res_c
        return y_pre[None].T
```
### 3. 模型测试
```python
model = NaiveBayes()
model.train(X, y)
test_X = np.array(['青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', 0.697, 0.460], dtype='object')
print(model.predict(test_X[None])[0][0])
```

是



## 7.4

**答：**由于$P(x_i|c)\leq 1$，因此$\prod_{i=1}^{d} P(x_i|c)$很可能因为非常接近$0$而造成下溢，可以采用对此公式求对数的方式将正小数转化为负正数从而解决数据可能下溢的问题，即转化为$\sum_{i=1}^{d} \log P(x_i|c)$。



## 7.5

**答：**对于贝叶斯最优分类器来说，其欲最大化的目标为
$$
P(c | \boldsymbol x) = \frac{P(c)P(\boldsymbol x | c)}{P(\boldsymbol x)}
$$

因为$P(\boldsymbol x)$是常量，因此也就是最大化
$$
P(c)P(\boldsymbol x | c)
$$
因为数据服从高斯分布，因此上式等价于
$$
P(c)\frac{1}{(2\pi)^{n/2}|\boldsymbol \Sigma |^{1/2} } \exp \left( -\frac12 (\boldsymbol x - \boldsymbol \mu_c)^\text T  \boldsymbol \Sigma_c^{-1} (\boldsymbol x - \boldsymbol \mu_c)\right)
$$
由题中条件“假设同先验”，因此在二分类任务中，上式等价于最大化
$$
\max(-\frac12 (\boldsymbol x - \boldsymbol \mu_0)^\text T  \boldsymbol \Sigma_0^{-1} (\boldsymbol x - \boldsymbol \mu_0), -\frac12 (\boldsymbol x - \boldsymbol \mu_1)^\text T  \boldsymbol \Sigma_1^{-1} (\boldsymbol x - \boldsymbol \mu_1))
$$
 由此可知，贝叶斯最优分类器的决策平面为
$$
\boldsymbol x ^ \text T \boldsymbol \Sigma^{-1}(\boldsymbol \mu_1 - \boldsymbol \mu_0) - \frac 12 (\boldsymbol \mu_1 + \boldsymbol \mu_0)^ \text T \boldsymbol\Sigma^{-1}(\boldsymbol \mu_1 - \boldsymbol \mu_0)
$$
对于线性判别分析来说，其欲最大化的目标为
$$
J = \frac{\boldsymbol{w}^\text{T}(\boldsymbol\mu_0-\boldsymbol\mu_1)(\boldsymbol\mu_0-\boldsymbol\mu_1)^\text T \boldsymbol{w}}{\boldsymbol{w} ^\text{T} (\boldsymbol\Sigma_0 + \boldsymbol\Sigma_1)\boldsymbol{w}}
$$
其中，$\boldsymbol w$为直线$y=\boldsymbol w ^ \text T \boldsymbol x$中的系数项，$\boldsymbol \mu_i, \boldsymbol \Sigma_i$为第$i \in \{0, 1\}$类的均值向量、协方差矩阵。由于两类数据满足高斯分布且方差相同，因此其决策平面由两数据均值点的中点$\frac 12 (\boldsymbol \mu_0 + \boldsymbol \mu_1)$和法向量$\boldsymbol w$唯一确定，而将此带入贝叶斯最优分类器的决策平面均成立，因此线性判别分析的决策平面与此重合。因此，二分类任务中两类数据满足高斯分布且方差相同时，线性判别分析产生贝叶斯最优分类器。




## 7.7

**答：**由题意，对于一个二分类问题，需要估算一个二值属性对应的先验概率$P(c, x_i)$则需要$30\times 2 \times 2=120$个样例（因为$c$和$x_i$的取值均有两种）。如果第二个二值属性在上述样例上对应的$c$和$x_i$的取值均相同，那么就无需增加样例；如果第二个二值属性在上述样例上的取值只有一个，那么还需要$30\times 2=60$个样例（另一个取值在$c$两取值上的个数）。

因此，当扩展到有$d$个二值属性时，最好情况不需要在第一个的基础上添加任何样例，因此样例数为$120$；最坏情况下，每一次添加属性均会添加$60$个样例，因此样例数为$120+60\times (d-1) = 60 \times (d + 1)$。

